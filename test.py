import torch
import esm
from Bio.Seq import Seq
import time
import random
from transformers import AutoTokenizer, EsmForProteinFolding


length = []
times = []
bp = ["A", "T", "G", "C"]
bp_seq = "ATAGACGCTATTGGGGGTATGATTGACCAGTTCACAGCAATGACAAATCGATCCTACTGAGTCTGCTGCATTAACTCATGTTCTTCCGACGGCGCATCTAGGGTTATGAGGCCTTGCGCGACCCACAAGCCGCAAGTAATTTGACACGTGCTAGCATACGAAACTAACCGTGCTGCCGGGACAAAGGTCGCACAAATGCGGTCGCTGCGCGGATGATCCTACAACAGCCCAAATACTATTTAAACTCGCTCCCGCAATGCTTAGGAGTACATGAATAGACACTACCTATAACTGGATTATGGATTGCCTCTTTCTCGTGAACCCGACGGTAGCAAAAACGCACCGTCAATGCATCGGAGGTCGTATTATTCCCCGTAACAATATTTATATCCATCCTCGCAGATCAGAGTAGGGTCGCGCGCTTAGCTCTAAATTCTAGATTTTATTCAATGGGGCGTTTGAATGGTGGTTATACAGGTTGGGTTGATAAGGCGGTAACAGGCATGCTACATTTGGCGTGATTTTCCAGGGGAACATGTTGGAGCTGGTGCCACGCGCCCTACTTTGTTACGTATAAAACGCTACTAGCGGACATAATTCCAGCGCCCTCTTAATAACGACGGGATTTGCAATGCGACGTTTGCATGCTTAGATAAGACCAGTCGCTAATAGGTTCACATCGTGTTATAAGGTTTACGGAGTCGGTACTCGAATATTTACGCAAGGGAACAAGGGCCAAGGGCCACCGCCGTCAGGCCATCTAGGAGTTGCCTGGTTAAATAGGAACATCAATCGTTTCTTGGTTGTGAGTTGTGTAAGTGATTGATGCCGTTGACTTCTCGAACGCAGCTTAATAAAGACAGTATAACCGGGCCTACATCCCCCCCCAGATGCGAGATATTGTGGCCGCCCAGAATTTGATTTGAATCCCTATGAAGAGTACGCGTTACAGTGTCGAGATGAATACTTTCTCGGGTTGAATCCACCTTGATAAAGACTCTCACGCGGACCGGTGGTACACGCCGTTATTAAACGAAACCGGTCTATAACAAGCTTCTATGCCGGTGGGTCGATCACATTATCGCAGGGCCCGTCGTAACAAGTACTGTCATTGGGGGATAAGCTCGATTGCCAAGCTTTTCCGCGTTTGGATGGGAGATGTGCGGCCTGTTCTGATGCGGGGTTTTTTTTCCTGACTAACTGTCGCGAATTGAACGCTTGTGTCAAACTGGCGGGTAAATCTTCGGTCACGGGTAAGGGGTTTTCGTATTCGCGCACCATGGCGAGCAGCATATGTCCTTATGAGGCTTTAAAGTCTTGACCTAGCGGGATCAGTGAGTTTAGTACGATCATTTGGAGAAAAGCATGGCGCTTTGTCAGTTTCCATATTTAATAGAACTCCAGTGAGTCATTCAACCGTCTGCTGCACTTCGTTAGCTTCTAACGATGCGAGTACGCGCCTACTGGCCGCGTATACCCTGCTGGTGGCTCAACCGTGACGGGACTCGCGGGCTGCTACGCACACGACCAGCAGCTTCTTGGTAATCGGTCACTCCCCGAATTCAATAGTTGTACGAAATACAACCCATTTAGACCGTTTGTGAATTTTCCGTAACACCTACACGACTTGCTGCCTTCGTGTATAAAGCGGGTCACCAAGCATTACCCCCATCCGTGATACATAAAGGCCATACATGTTATATACCTAACGGCATAATGGCTACCTAGTTTCTCGTCCTAGATCAAATGGCCAGGGATAGCATCAGCACGGAAGATGCTCAGTTTCAATAAGGACACAGTTCTAGGACCCTGTGCTCCCAGTTCGACTGAAGACTCATTTTGTTCAGTTATTATTCGATCTCATAGTTCGCGTACGCACATGGTGATAGCTGTCCCAAAGATTCTCGTATGGCTGATCGTTCAACACACGACATTTTGAATAACACCGATATTGATCCCGCATCGTACTTGCCAGGTCCTACTCATGAGTGGTAACGAGG"

print("getting model")
tokenizer = AutoTokenizer.from_pretrained("facebook/esmfold_v1")
model = EsmForProteinFolding.from_pretrained("facebook/esmfold_v1")

model.esm = model.esm.half()
model = model.cuda()
torch.backends.cuda.matmul.allow_tf32 = True
model.trunk.set_chunk_size(64)

while len(bp_seq)< 100000:
    print(f"predicting for length of {len(bp_seq)}")
    # translate to protein
    # use both fwd and rev sequences
    dna_record = Seq(bp_seq)
    dna_seqs = [dna_record, dna_record.reverse_complement()]

    # generate all translation frames
    data = [str(s[i:].translate(to_stop=True)) for i in range(3) for s in dna_seqs]

    print("predicting")
    current = time.process_time()
    for seq in data:
        if len(seq) > 0:
            tokenized_input = tokenizer([seq], return_tensors="pt", add_special_tokens=False)["input_ids"]
            tokenized_input = tokenized_input.cuda()

            # Extract per-residue representations (on CPU)
            with torch.no_grad():
                notebook_prediction = model.infer_pdb(seq)
    times.append(time.process_time()-current)

    length.append(len(dna_record))
    
    import csv

    with open('results>2000.csv', 'w') as f:
        writer = csv.writer(f)
        writer.writerows(zip(length, times))
    
    # extend by 100 codons
    for new_bp in random.choices(bp, k=300):
        bp_seq += new_bp
